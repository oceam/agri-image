{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "weed_classification_01",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oceam/agri-image/blob/main/codes/weed_classification_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "weed_classification_01  \n",
        "Masanori Ishii & Wei Guo  \n",
        "2022.08.26"
      ],
      "metadata": {
        "id": "KJxJq4P9Ln0o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op_4dXNe5oEV"
      },
      "source": [
        "# Build a Weed Discriminator by image clasification model  \n",
        "  We prepared a weed dataset that include 10 species of weeds  \n",
        "\n",
        "雑草の生育期間を区別せずに分類器を作る（10種類）  \n",
        "　雑草の生育期間（芽生え・生育済み）を区別せずに分類器を作成します。 育成した雑草の種類はハキダメギク、ホソアオゲイトウ、イチビ、イヌビエ、コセンダングサ、マメアサガオ、メヒシバ、オヒシバ、オイヌタデ、シロザの10種類です"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCQh_T0zLntu"
      },
      "source": [
        "### Download the Dataset\n",
        "データのダウンロード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jdbMuOmSpjy"
      },
      "source": [
        "download weeds_1.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkXy31etIQpT"
      },
      "source": [
        "import gdown\n",
        "# weeds_1\n",
        "url = \"https://drive.google.com/file/d/1w8UOD_vFWEzL3MYr4Gl1oF-Fiy7iklUW/view?usp=sharing\"\n",
        "output= \"weeds_1.zip\"\n",
        "gdown.download(url=url, output=output, quiet=False, fuzzy=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKmeWPABPKpp"
      },
      "source": [
        "weeds_1.zipを解凍します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAC7SLKpJqEl"
      },
      "source": [
        "!unzip weeds_1.zip\n",
        "print(\"unzip weeds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzF0mQNICl6x"
      },
      "source": [
        "### Prepare the dataset\n",
        "データセットの準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWEdpQ6tv4G"
      },
      "source": [
        "Display the images of weeds dataset  (\"sprout\", \"grown\")  \n",
        "データセットの画像を表示します（芽生え・生育済み）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp5n6m-AM6cX"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def show_img():\n",
        "  #　雑草名と生育状態をリストに定義\n",
        "  # weed_names = os.listdir('weeds_1/')\n",
        "  # weed_types = os.listdir(os.path.join('weeds_1/', weed_names[0]))\n",
        "  weed_names = [\"hakidamegiku\",\"hosoaogeitou\",\"ichibi\",\"inubie\",\"kosendangusa\",\"mameasagao\",\"mehishiba\",\"ohishiba\",\"oinutade\",\"shiroza\"]\n",
        "  weed_types = [\"sprout\", \"grown\"]\n",
        "\n",
        "  #　データ格納フォルダを指定\n",
        "  input_dir = \"./weeds_1\"\n",
        "\n",
        "  #　リストの長さを足して表示枚数を確認\n",
        "  hs = len(weed_names)*len(weed_types)\n",
        "\n",
        "  #　表示設定\n",
        "  col=len(weed_names)\n",
        "  row=hs/col\n",
        "  cols=col*4\n",
        "  rows=row*4\n",
        "  dpis = 100\n",
        "\n",
        "  #　イメージの表示サイズ、解像度\n",
        "  fig = plt.figure(figsize=(cols,rows),dpi=dpis)\n",
        "  \n",
        "  #　＊＊番目に指定\n",
        "  pi=1\n",
        "\n",
        "  #　イメージ表示\n",
        "  for weed_name in weed_names:\n",
        "    #　1段目\n",
        "    img_path = os.path.join(input_dir, weed_name, weed_types[0])\n",
        "    img_list = os.listdir(img_path)\n",
        "    plot_num = pi\n",
        "    ax=fig.add_subplot(row, col, plot_num)\n",
        "    ax.set_title(weed_name, fontsize=20)\n",
        "    if plot_num == 1:\n",
        "      plt.ylabel(weed_types[0], fontsize=20) # y軸ラベル\n",
        "    img = Image.open(os.path.join(img_path, img_list[6])) # indexを変更して別の画像を表示！！\n",
        "    plt.xticks(color=\"None\")\n",
        "    plt.yticks(color=\"None\")\n",
        "    plt.tick_params(length=0)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "  #　2段目\n",
        "    img_path = os.path.join(input_dir, weed_name, weed_types[1])\n",
        "    img_list = os.listdir(img_path)\n",
        "    plot_num2 = pi+10\n",
        "    bx=fig.add_subplot(row, col, plot_num2)\n",
        "    bx.set_title(weed_name, fontsize=20, pad=0)\n",
        "    if plot_num2 == 11:\n",
        "      plt.ylabel(weed_types[1], fontsize=20) # y軸ラベル\n",
        "    img2 = Image.open(os.path.join(img_path, img_list[2]))\n",
        "    plt.xticks(color=\"None\")\n",
        "    plt.yticks(color=\"None\")\n",
        "    plt.tick_params(length=0)\n",
        "    plt.imshow(img2, cmap='gray')\n",
        "    pi = pi+1\n",
        "  fig.align_labels()\n",
        "\n",
        "show_img()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13kgHJHOTLy9"
      },
      "source": [
        "Generate the directory of train、validation、prediction  \n",
        "train、validation、prediction用のディレクトリを作成し、class用のディレクトリを追加します"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# debug use\n",
        "# import shutil\n",
        "# shutil.rmtree('CLS/', ignore_errors=True)"
      ],
      "metadata": {
        "id": "1cN2BtNFQWvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zsW_yV4blgv"
      },
      "source": [
        "import os, shutil\n",
        "\n",
        "# make directory CLS\n",
        "base_dir = \"./CLS\"\n",
        "if \"CLS\" not in os.listdir(\"./\"):\n",
        "  os.mkdir(base_dir)\n",
        "else:\n",
        "  print(base_dir, \"already exists\")\n",
        "\n",
        "# make directory train\n",
        "train_index = \"train\"\n",
        "train_dir = os.path.join(base_dir, train_index)\n",
        "if train_index not in os.listdir(base_dir):\n",
        "  os.mkdir(train_dir)\n",
        "else:\n",
        "  print(train_dir + \"already exists\")\n",
        "\n",
        "# make directory validation\n",
        "validation_index = \"validation\"\n",
        "validation_dir = os.path.join(base_dir, validation_index)\n",
        "if validation_index not in os.listdir(base_dir):\n",
        "  os.mkdir(validation_dir)\n",
        "else:\n",
        "  print(validation_dir + \"already exists\")\n",
        "\n",
        "# make directory prediction\n",
        "prediction_index = \"prediction\"\n",
        "prediction_dir = os.path.join(base_dir, prediction_index)\n",
        "if prediction_index not in os.listdir(base_dir):\n",
        "  os.mkdir(prediction_dir)\n",
        "else:\n",
        "  print(prediction_dir + \"already exists\")\n",
        "\n",
        "# make directory class\n",
        "# weed_names = os.listdir('weeds_1/')\n",
        "# weed_types = os.listdir(os.path.join('weeds_1/', weed_names[0]))\n",
        "# classes=weed_names\n",
        "weed_names = [\"hakidamegiku\",\"hosoaogeitou\",\"ichibi\",\"inubie\",\"kosendangusa\",\"mameasagao\",\"mehishiba\",\"ohishiba\",\"oinutade\",\"shiroza\"]\n",
        "weed_types = [\"sprout\", \"grown\"]\n",
        "classes=[\"hakidamegiku\",\"hosoaogeitou\",\"ichibi\",\n",
        "         \"inubie\",\"kosendangusa\",\"mameasagao\",\n",
        "         \"mehishiba\",\"ohishiba\",\"oinutade\",\"shiroza\",]\n",
        "dirs = os.listdir(base_dir)\n",
        "for dir in dirs:\n",
        "  for cls in classes:\n",
        "    # Directory with our training pictures\n",
        "    class_dir = os.path.join(base_dir, dir, cls)\n",
        "    if cls not in os.listdir(base_dir + \"/\" + dir):\n",
        "      os.mkdir(class_dir)\n",
        "    else:\n",
        "      print(class_dir, \"already exists\")\n",
        "\n",
        "print(\"finished make directory\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CPwpwvHcAOZ"
      },
      "source": [
        "Split images to directory  \n",
        "画像をディレクトリに振り分けます"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from os.path import join\n",
        "clsdir = \"./weeds_1\"\n",
        "base_dir = \"./CLS\"\n",
        "#weed_names = [\"hakidamegiku\",\"hosoaogeitou\",\"ichibi\",\"inubie\",\"kosendangusa\",\"mameasagao\",\"mehishiba\",\"ohishiba\",\"oinutade\",\"shiroza\"]\n",
        "#weed_types = [\"sprout\", \"grown\"]\n",
        "rads=np.arange(0,200)\n",
        "np.random.shuffle(rads)\n",
        "#print(rads)\n",
        "train_idx, val_idx,test_idx=np.split(rads, [int(.7 * len(rads)), int(.9 * len(rads))])\n",
        "for weed_name in weed_names:\n",
        "  for weed_type in weed_types:\n",
        "    file_names = os.listdir(os.path.join(clsdir, weed_name, weed_type))\n",
        "    for idx in train_idx:\n",
        "        # 移動元のファイル\n",
        "        path1 = os.path.join(clsdir, weed_name, weed_type, file_names[idx])\n",
        "        # 移動先のファイル\n",
        "        path2= os.path.join(base_dir, \"train\", weed_name, file_names[idx])\n",
        "        # ファイルを移動\n",
        "        new_path = shutil.copy(path1, path2)\n",
        "        #print('copied training data')            \n",
        "    for idx in val_idx:\n",
        "        # 移動元のファイル\n",
        "        path1 = os.path.join(clsdir, weed_name, weed_type, file_names[idx])\n",
        "        # 移動先のファイル\n",
        "        path2= os.path.join(base_dir, \"validation\", weed_name, file_names[idx])\n",
        "        # ファイルを移動\n",
        "        new_path = shutil.copy(path1, path2)\n",
        "        #print('copied vilidation data') \n",
        "    for idx in test_idx:\n",
        "        # 移動元のファイル\n",
        "        path1 = os.path.join(clsdir, weed_name, weed_type, file_names[idx])\n",
        "        # 移動先のファイル\n",
        "        path2= os.path.join(base_dir, \"prediction\", weed_name, file_names[idx])\n",
        "        # ファイルを移動\n",
        "        new_path = shutil.copy(path1, path2)\n",
        "        #print('copied test data')    \n",
        "print('finished copy')      "
      ],
      "metadata": {
        "id": "1GYvV8zaG1pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_nums=len(os.listdir('CLS/train/ichibi'))\n",
        "print('number of train image',train_nums)\n",
        "val_nums=len(os.listdir('CLS/validation/ichibi'))\n",
        "print('number of train image',val_nums)\n",
        "test_nums=len(os.listdir('CLS/prediction/ichibi'))\n",
        "print('number of test image',test_nums)"
      ],
      "metadata": {
        "id": "1rkM8iiPK5or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89zSPJPufnsz"
      },
      "source": [
        "・trainデータ、validationデータ、predictionデータのgeneratorを作成します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ork4z7VlHv5T"
      },
      "source": [
        "import os \n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras.preprocessing.image as Image\n",
        "\n",
        "input_size = 224\n",
        "\n",
        "train_dir = \"./CLS/train\"\n",
        "validation_dir = \"./CLS/validation\"\n",
        "\n",
        "\n",
        "train_datagen = Image.ImageDataGenerator(\n",
        "            featurewise_center = False,\n",
        "            samplewise_center = False,\n",
        "            featurewise_std_normalization = False,\n",
        "            samplewise_std_normalization = False,\n",
        "            zca_whitening = False,\n",
        "            rotation_range = 90,\n",
        "            width_shift_range = 0.3,\n",
        "            height_shift_range = 0.3,\n",
        "            horizontal_flip = True,\n",
        "            vertical_flip = False,\n",
        "            rescale=1./255\n",
        "        )\n",
        "\n",
        "val_datagen = Image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "            train_dir,\n",
        "            target_size=(input_size,input_size),\n",
        "            batch_size=10,\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "            validation_dir,\n",
        "            target_size=(input_size,input_size),\n",
        "            batch_size=10,\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "print(\"dataset prepared\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnbD3y_Zh7Pg"
      },
      "source": [
        "### ■トレーニングの実行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR5MXYEeWvGq"
      },
      "source": [
        "・モデルのレイヤー構成を定義します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dq94WdxULw9"
      },
      "source": [
        "\n",
        "#ファインチューニング+VGG+水増し。ここから実行してOK（VGG16をダウンロード）\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.optimizers import adam_v2\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.applications.vgg16 import VGG16\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import History, Callback\n",
        "# from keras.objectives import categorical_crossentropy\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "# from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from scipy.stats import mode\n",
        "import os, pickle\n",
        "\n",
        "def create_cnn():\n",
        "  input_size=224\n",
        "  #input_sizeは224,224までOK。\n",
        "\n",
        "  vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(input_size,input_size, 3))\n",
        "  last = vgg_conv.output\n",
        "\n",
        "  vgg_conv.trainable = True\n",
        "\n",
        "  set_trainable = False\n",
        "  for layer in vgg_conv.layers:\n",
        "    if layer.name == 'block5_conv1':\n",
        "      set_trainable = True\n",
        "    if set_trainable:\n",
        "      layer.trainable = True\n",
        "    else:\n",
        "      layer.trainable = False\n",
        "\n",
        "  mod = Flatten()(last)\n",
        "  mod = Dense(256, activation='relu')(mod)\n",
        "  #mod = Dropout(0.5)(mod)\n",
        "  preds = Dense(10, activation='softmax')(mod)\n",
        "\n",
        "  model = models.Model(vgg_conv.input, preds)\n",
        "\n",
        "  return model\n",
        "print(\"レイヤー構成を定義しました！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMCP246j39AB"
      },
      "source": [
        "・チェックポイントを定義します(val lossが一番低い値の時にweightファイルを保存)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa68Qz4p3_Ov"
      },
      "source": [
        "class Checkpoint(Callback):\n",
        "    def __init__(self, model, filepath):\n",
        "        self.model = model\n",
        "        self.filepath = filepath\n",
        "        self.best_val_acc = 0.0\n",
        "        self.best_val_loss = 0.7\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        # val_lossが最小の時ににweightを保存する\n",
        "        if self.best_val_loss > logs[\"val_loss\"]:\n",
        "            self.model.save_weights(self.filepath)\n",
        "            self.best_val_loss = logs[\"val_loss\"]\n",
        "            print(\"Weights saved.\", self.best_val_loss)\n",
        "print(\"チェックポイントを定義しました！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-DwQxyRfdxG"
      },
      "source": [
        "・学習の実行手順を定義します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLkWD57CZJEX"
      },
      "source": [
        "def train():\n",
        "    print(\"学習を開始します\")\n",
        "    hist = History()\n",
        "    train_model = create_cnn()\n",
        "    train_model.compile(optimizer=adam_v2.Adam(learning_rate=1e-5),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    cp = Checkpoint(train_model, f\"weights.hdf5\")\n",
        "    train_model.fit_generator(train_generator,epochs=10,validation_data=validation_generator,callbacks=[hist, cp])      \n",
        "    print(\"学習が完了しました\")\n",
        "    return hist.history\n",
        "print(\"実行手順を定義しました！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjCJKxdZXbbK"
      },
      "source": [
        "・学習を開始します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqaVKLXZgiEZ"
      },
      "source": [
        "K.clear_session()\n",
        "hist = train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXMQMfNufvlL"
      },
      "source": [
        "### ■正解率と損失率をグラフ化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP7_j_XPZY6m"
      },
      "source": [
        "・trainの正解率と損失率、validationの正解率と損失率をグラフ化します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tik0x-QsZKtC"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = hist\n",
        "\n",
        "acc=history['accuracy']\n",
        "val_acc=history['val_accuracy']\n",
        "loss=history['loss']\n",
        "val_loss=history['val_loss']\n",
        "\n",
        "epochs=range(1,len(acc)+1)\n",
        "\n",
        "#正解率plot\n",
        "plt.plot(epochs,acc,'bo',label='Training acc')\n",
        "plt.plot(epochs,val_acc,'b',label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "#損失値をplot\n",
        "plt.plot(epochs,loss,'bo',label='Training loss')\n",
        "plt.plot(epochs,val_loss,'b',label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KRPYAVR_9Zn"
      },
      "source": [
        "### ■テスト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts0ZYstl6x4y"
      },
      "source": [
        "・prediction用のデータセットを作成します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNZBgLlClER7"
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob\n",
        "import numpy as np\n",
        "from PIL import ImageFile\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES =True\n",
        "\n",
        "prediction_dir = \"./CLS/prediction\"\n",
        "prediction_classes = [\"hakidamegiku\",\"hosoaogeitou\",\"ichibi\",\"inubie\",\"kosendangusa\",\"mameasagao\",\"mehishiba\",\"ohishiba\",\"oinutade\",\"shiroza\"]\n",
        "\n",
        "image_size = 224\n",
        "print(prediction_classes)\n",
        "X_test = []\n",
        "y_test = []\n",
        "for index, classlabel in enumerate(prediction_classes):\n",
        "    photos_dir = os.path.join(prediction_dir, classlabel)\n",
        "    files = glob.glob(photos_dir + \"/*.JPG\")\n",
        "    print(files)\n",
        "    for i, file in enumerate(files):\n",
        "        image = Image.open(file)\n",
        "        image = image.convert(\"RGB\")\n",
        "        image = image.resize((image_size, image_size))\n",
        "        data = np.asarray(image)\n",
        "        if i == 0:\n",
        "          print(data.shape)\n",
        "        X_test.append(data)\n",
        "        y_test.append(index)\n",
        "\n",
        "X_test1 = np.array(X_test)\n",
        "y_test1 = np.array(y_test)\n",
        "print(\"predictionデータ作成完了！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJiCyfYW8GKy"
      },
      "source": [
        "・混同行列(Confusion Matrix)表示用の関数を定義します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGZMSUefz-C5"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    # print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=-90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "print(\"混同行列表示用の関数を定義しました！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVEpU5mMEEOi"
      },
      "source": [
        "・保存したweightファイルをロードしてテストを行います"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sin_predict():\n",
        "    X_test, y_test = X_test1, y_test1\n",
        "    X_test = X_test / 255.0\n",
        "    y_test_label = np.ravel(y_test)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    train_model = create_cnn()\n",
        "    train_model.compile(optimizer=adam_v2.Adam(learning_rate=1e-5),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    # 最良のモデルの読み込み\n",
        "    train_model.load_weights(f\"weights.hdf5\")\n",
        "    for layer in train_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "    # 単体のテスト\n",
        "    single_pred = np.argmax(train_model.predict(X_test), axis=-1)\n",
        "\n",
        "    # テストのスコア\n",
        "    test_acc = accuracy_score(y_test, to_categorical(single_pred))\n",
        "\n",
        "    print(\"テストの結果は\", test_acc, \"です\")\n",
        "\n",
        "    target_names = [\"hakidamegiku\",\"hosoaogeitou\",\"ichibi\",\"inubie\",\"kosendangusa\",\"mameasagao\",\"mehishiba\",\"ohishiba\",\"oinutade\",\"shiroza\"]\n",
        "    cm = confusion_matrix(y_test_label, single_pred)\n",
        "    plot_confusion_matrix(cm, classes = target_names) \n",
        "print(\"テスト用の関数を定義しました！\")    "
      ],
      "metadata": {
        "id": "HLTTTKJUUCHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# テスト\n",
        "sin_predict()"
      ],
      "metadata": {
        "id": "E0JiHw1Y3ZL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b340YPV35B--"
      },
      "source": [
        "### ■任意の写真をテストします"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "343MbhcH5lUv"
      },
      "source": [
        "・画像テスト用の関数を定義します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTKPufr6xSGa"
      },
      "source": [
        "def result_predict(path):\n",
        "    prediction_classes = [\"hakidamegiku\",\"hosoaogeitou\",\"ichibi\",\"inubie\",\"kosendangusa\",\"mameasagao\",\"mehishiba\",\"ohishiba\",\"oinutade\",\"shiroza\"]\n",
        "\n",
        "    train_model = create_cnn()\n",
        "    train_model.compile(optimizer=adam_v2.Adam(lr=1e-5),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    # 最良のモデルの読み込み\n",
        "    train_model.load_weights(f\"weights.hdf5\")\n",
        "    for layer in train_model.layers:\n",
        "        layer.trainable = False  \n",
        "    \n",
        "    X_test = []\n",
        "    image_size = 224\n",
        "    image = Image.open(path)\n",
        "    image = image.convert(\"RGB\")\n",
        "    image = image.resize((image_size, image_size))\n",
        "    data = np.asarray(image)\n",
        "    X_test.append(data)\n",
        "    X_test = np.array(X_test) / 255.0\n",
        "    result = np.argmax(train_model.predict(X_test), axis=-1)\n",
        "    print(\"This picture is :\", prediction_classes[result[0]])\n",
        "    print(\"雑草の種類は :\", prediction_classes[result[0]])\n",
        "print(\"defined function for testing！\")\n",
        "print(\"画像テスト用の関数を定義しました！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gLtWRQr5Z_m"
      },
      "source": [
        "・predictionディレクトリから任意の写真を選択しpathを設定します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMTohBSJyDMc"
      },
      "source": [
        "image_dir = \"/content/CLS/prediction/ichibi/ichibi_IMG_4165_10.JPG\"\n",
        "result_predict(image_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}