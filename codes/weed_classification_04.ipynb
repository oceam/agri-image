{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oceam/agri-image/blob/main/codes/weed_classification_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "weed_classification_04\n",
        "Masanori Ishii & Wei Guo  \n",
        "2022.08.26"
      ],
      "metadata": {
        "id": "HlsCV0B-K_PA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op_4dXNe5oEV"
      },
      "source": [
        "\n",
        "# Build a Weed Discriminator by image clasification model (with consideration of growth stage and category)  \n",
        "科目が同じ品種をグループにして分類器を作る（生育済み）\n",
        "雑草の生育期間が生育済みのデータを用いて分類器を作成します。 ハキダメギク、ホソアオゲイトウ、イチビ、イヌビエ、コセンダングサ、マメアサガオ、メヒシバ、オヒシバ、オイヌタデ、シロザの10種類の雑草のうち科目が同じのものを一つのclassにまとめるます。イネ科（イヌビエ、メヒシバ、オヒシバ）、キク科（ハキダメギク、コセンダングサ）とその他5種類に分けて分類器を作成します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H30Zi_abswHO"
      },
      "source": [
        "### ■データのダウンロード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECZ80CAEUUOK"
      },
      "source": [
        "・cluster.zipをダウンロードします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkXy31etIQpT"
      },
      "source": [
        "import gdown\n",
        "# weeds_1\n",
        "url = \"https://drive.google.com/file/d/1w8UOD_vFWEzL3MYr4Gl1oF-Fiy7iklUW/view?usp=sharing\"\n",
        "output= \"weeds_1.zip\"\n",
        "gdown.download(url=url, output=output, quiet=False, fuzzy=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KHyYz65Ujwl"
      },
      "source": [
        "・cluster.zipを解凍します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lvh2h6YBLf0"
      },
      "source": [
        "!unzip weeds_1\n",
        "print(\"weeds_1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8RuGXpM6q0y"
      },
      "source": [
        "### ■データセットの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzF0mQNICl6x"
      },
      "source": [
        "・データセットの画像を表示します（イネ科、キク科、その他）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp5n6m-AM6cX"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def show_weed():\n",
        "  #　雑草名と生育状態をリストに定義\n",
        "  weed_names = [\"hakidamegiku\",\"hosoaogeitou\",\"ichibi\",\"inubie\",\"kosendangusa\",\"mameasagao\",\"mehishiba\",\"ohishiba\",\"oinutade\",\"shiroza\"]\n",
        "  weed_categoly = {\"hakidamegiku\":\"kiku\",\"hosoaogeitou\":\"hosoaogeitou\",\"ichibi\":\"ichibi\",\n",
        "         \"inubie\":\"grass\",\"kosendangusa\":\"kiku\",\"mameasagao\":\"mameasagao\",\n",
        "         \"mehishiba\":\"grass\",\"ohishiba\":\"grass\",\"oinutade\":\"oinutade\",\"shiroza\":\"shiroza\"}\n",
        "  weed_type = [\"sprout\", \"grown\"]\n",
        "\n",
        "  #　データ格納フォルダを指定\n",
        "  input_dir = \"./weeds_1\"\n",
        "\n",
        "  #　リストの長さを足して表示枚数を確認\n",
        "  hs = 15\n",
        "\n",
        "  #　表示設定\n",
        "  col= 5\n",
        "  row=hs/col\n",
        "  cols=col*4\n",
        "  rows=row*4\n",
        "  dpis = 100\n",
        "\n",
        "  #　イメージの表示サイズ、解像度\n",
        "  fig = plt.figure(figsize=(cols,rows),dpi=dpis)\n",
        "  \n",
        "  #　＊＊番目に指定\n",
        "  pi1=1\n",
        "  pi2=6\n",
        "  pi3=11\n",
        "\n",
        "  #　grassイメージ表示\n",
        "  for weed_name in weed_names:\n",
        "    if weed_categoly[weed_name] == \"grass\":\n",
        "      img_path = os.path.join(input_dir, weed_name, weed_type[1])\n",
        "      img_list = os.listdir(img_path)\n",
        "      plot_num = pi1\n",
        "      ax=fig.add_subplot(row, col, plot_num)\n",
        "      ax.set_title(weed_name, fontsize=20)\n",
        "      if plot_num == 1:\n",
        "        plt.ylabel(\"grass\", fontsize=20) # y軸ラベル\n",
        "      img = Image.open(os.path.join(img_path, img_list[2])) # indexを変更して別の画像を表示！！\n",
        "      plt.xticks(color=\"None\")\n",
        "      plt.yticks(color=\"None\")\n",
        "      plt.tick_params(length=0)\n",
        "      plt.imshow(img, cmap='gray')\n",
        "      pi1 = pi1+1\n",
        "    elif weed_categoly[weed_name] == \"kiku\":\n",
        "      img_path = os.path.join(input_dir, weed_name, weed_type[1])\n",
        "      img_list = os.listdir(img_path)\n",
        "      plot_num = pi2\n",
        "      ax=fig.add_subplot(row, col, plot_num)\n",
        "      ax.set_title(weed_name, fontsize=20)\n",
        "      if plot_num == 6:\n",
        "        plt.ylabel(\"kiku\", fontsize=20) # y軸ラベル\n",
        "      img = Image.open(os.path.join(img_path, img_list[2])) # indexを変更して別の画像を表示！！\n",
        "      plt.xticks(color=\"None\")\n",
        "      plt.yticks(color=\"None\")\n",
        "      plt.tick_params(length=0)\n",
        "      plt.imshow(img, cmap='gray')\n",
        "      pi2 = pi2+1\n",
        "    else:\n",
        "      img_path = os.path.join(input_dir, weed_name, weed_type[1])\n",
        "      img_list = os.listdir(img_path)\n",
        "      plot_num = pi3\n",
        "      ax=fig.add_subplot(row, col, plot_num)\n",
        "      ax.set_title(weed_name, fontsize=20)\n",
        "      if plot_num == 11:\n",
        "        plt.ylabel(\"others\", fontsize=20) # y軸ラベル\n",
        "      img = Image.open(os.path.join(img_path, img_list[2])) # indexを変更して別の画像を表示！！\n",
        "      plt.xticks(color=\"None\")\n",
        "      plt.yticks(color=\"None\")\n",
        "      plt.tick_params(length=0)\n",
        "      plt.imshow(img, cmap='gray')\n",
        "      pi3 = pi3+1\n",
        "\n",
        "  fig.align_labels()\n",
        "\n",
        "show_weed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I5tYVdatNel"
      },
      "source": [
        "・train、validation、prediction用のディレクトリを作成し、class用のディレクトリを追加します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zsW_yV4blgv"
      },
      "source": [
        "import os, shutil\n",
        "\n",
        "# The directory where we will\n",
        "# store our smaller dataset\n",
        "base_dir = \"./CLS\"\n",
        "if \"CLS\" not in os.listdir(\"./\"):\n",
        "  os.mkdir(base_dir)\n",
        "else:\n",
        "  print(base_dir, \"は既に存在します\")\n",
        "\n",
        "train_index = \"train\"\n",
        "train_dir = os.path.join(base_dir, train_index)\n",
        "if train_index not in os.listdir(base_dir):\n",
        "  os.mkdir(train_dir)\n",
        "else:\n",
        "  print(train_dir + \"は既に存在します\")\n",
        "\n",
        "validation_index = \"validation\"\n",
        "validation_dir = os.path.join(base_dir, validation_index)\n",
        "if validation_index not in os.listdir(base_dir):\n",
        "  os.mkdir(validation_dir)\n",
        "else:\n",
        "  print(validation_dir + \"は既に存在します\")\n",
        "\n",
        "prediction_index = \"prediction\"\n",
        "prediction_dir = os.path.join(base_dir, prediction_index)\n",
        "if prediction_index not in os.listdir(base_dir):\n",
        "  os.mkdir(prediction_dir)\n",
        "else:\n",
        "  print(prediction_dir + \"は既に存在します\")\n",
        "\n",
        "#7分類のclassを入力\n",
        "classes=[\"kiku\",\"hosoaogeitou\",\"ichibi\",\n",
        "         \"grass\",\"mameasagao\",\n",
        "         \"oinutade\",\"shiroza\",]\n",
        "dirs = os.listdir(base_dir)\n",
        "for dir in dirs:\n",
        "  for cls in classes:\n",
        "    # Directory with our training pictures\n",
        "    class_dir = os.path.join(base_dir, dir, cls)\n",
        "    if cls not in os.listdir(base_dir + \"/\" + dir):\n",
        "      os.mkdir(class_dir)\n",
        "    else:\n",
        "      print(class_dir, \"は既に存在します\")\n",
        "\n",
        "print(\"作成完了！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CPwpwvHcAOZ"
      },
      "source": [
        "・画像をディレクトリに振り分けます"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06Htjh2XbM50"
      },
      "source": [
        "from os.path import join\n",
        "import random\n",
        "\n",
        "# クラス作成用の品種の名前を定義\n",
        "classes = {\"hakidamegiku\":\"kiku\",\"hosoaogeitou\":\"hosoaogeitou\",\"ichibi\":\"ichibi\",\n",
        "         \"inubie\":\"grass\",\"kosendangusa\":\"kiku\",\"mameasagao\":\"mameasagao\",\n",
        "         \"mehishiba\":\"grass\",\"ohishiba\":\"grass\",\"oinutade\":\"oinutade\",\"shiroza\":\"shiroza\"}\n",
        "\n",
        "clsdir = \"./weeds_1\"\n",
        "base_dir = \"./CLS\"\n",
        "# weed_typeを生育済みのみに設定\n",
        "weed_types = [\"grown\"]\n",
        "weed_names = os.listdir(clsdir)\n",
        "# ファイル（クラスごとに分かれている）を順番に読み取り\n",
        "# train, validationデータを作成\n",
        "for tra_val_dir in [\"train\", \"validation\"]:\n",
        "  for weed_name in weed_names:\n",
        "    for weed_type in weed_types:\n",
        "      # grass(イネ科)    \n",
        "      if classes[weed_name] == \"grass\": \n",
        "        print(weed_name)\n",
        "        file_names = os.listdir(os.path.join(clsdir, weed_name, weed_type))\n",
        "        files30 = random.sample(file_names, int(14))\n",
        "        for file_name in file_names:\n",
        "          if  file_name in files30:\n",
        "            # 移動元のファイル\n",
        "            path1 = os.path.join(clsdir, weed_name, weed_type, file_name)\n",
        "            # 移動先のファイル\n",
        "            path2= os.path.join(base_dir, tra_val_dir, classes[weed_name], file_name)\n",
        "            # ファイルを移動\n",
        "            new_path = shutil.move(path1, path2)\n",
        "            # ファイルの存在確認\n",
        "            print(os.path.exists(path2))\n",
        "      # kiku(キク科)    \n",
        "      elif classes[weed_name] == \"kiku\": \n",
        "        print(weed_name)\n",
        "        file_names = os.listdir(os.path.join(clsdir, weed_name, weed_type))\n",
        "        files45 = random.sample(file_names, int(21))\n",
        "        for file_name in file_names:\n",
        "          if  file_name in files45:\n",
        "            # 移動元のファイル\n",
        "            path1 = os.path.join(clsdir, weed_name, weed_type, file_name)\n",
        "            # 移動先のファイル\n",
        "            path2= os.path.join(base_dir, tra_val_dir, classes[weed_name], file_name)\n",
        "            # ファイルを移動\n",
        "            new_path = shutil.move(path1, path2)\n",
        "            # ファイルの存在確認\n",
        "            print(os.path.exists(path2))\n",
        "      # それ以外   \n",
        "      else: \n",
        "        print(weed_name)\n",
        "        file_names = os.listdir(os.path.join(clsdir, weed_name, weed_type))\n",
        "        files90 = random.sample(file_names, int(42))\n",
        "        for file_name in file_names:\n",
        "          if  file_name in files90:\n",
        "            # 移動元のファイル\n",
        "            path1 = os.path.join(clsdir, weed_name, weed_type, file_name)\n",
        "            # 移動先のファイル\n",
        "            path2= os.path.join(base_dir, tra_val_dir, classes[weed_name], file_name)\n",
        "            # ファイルを移動\n",
        "            new_path = shutil.move(path1, path2)\n",
        "            # ファイルの存在確認\n",
        "            print(os.path.exists(path2))\n",
        "\n",
        "# predictionデータを作成\n",
        "for tra_val_dir in [\"prediction\"]:\n",
        "  for weed_name in weed_names:\n",
        "    for weed_type in weed_types:\n",
        "      # grass(イネ科)    \n",
        "      if classes[weed_name] == \"grass\": \n",
        "        print(weed_name)\n",
        "        file_names = os.listdir(os.path.join(clsdir, weed_name, weed_type))\n",
        "        files06 = random.sample(file_names, int(4))\n",
        "        for file_name in file_names:\n",
        "          if  file_name in files06:\n",
        "            # 移動元のファイル\n",
        "            path1 = os.path.join(clsdir, weed_name, weed_type, file_name)\n",
        "            # 移動先のファイル\n",
        "            path2= os.path.join(base_dir, tra_val_dir, classes[weed_name], file_name)\n",
        "            # ファイルを移動\n",
        "            new_path = shutil.move(path1, path2)\n",
        "            # ファイルの存在確認\n",
        "            print(os.path.exists(path2))\n",
        "      # kiku(キク科)    \n",
        "      elif classes[weed_name] == \"kiku\": \n",
        "        print(weed_name)\n",
        "        file_names = os.listdir(os.path.join(clsdir, weed_name, weed_type))\n",
        "        files09 = random.sample(file_names, int(6))\n",
        "        for file_name in file_names:\n",
        "          if  file_name in files09:\n",
        "            # 移動元のファイル\n",
        "            path1 = os.path.join(clsdir, weed_name, weed_type, file_name)\n",
        "            # 移動先のファイル\n",
        "            path2= os.path.join(base_dir, tra_val_dir, classes[weed_name], file_name)\n",
        "            # ファイルを移動\n",
        "            new_path = shutil.move(path1, path2)\n",
        "            # ファイルの存在確認\n",
        "            print(os.path.exists(path2))\n",
        "      # それ以外   \n",
        "      else: \n",
        "        print(weed_name)\n",
        "        file_names = os.listdir(os.path.join(clsdir, weed_name, weed_type))\n",
        "        files18 = random.sample(file_names, int(12))\n",
        "        for file_name in file_names:\n",
        "          if  file_name in files18:\n",
        "            # 移動元のファイル\n",
        "            path1 = os.path.join(clsdir, weed_name, weed_type, file_name)\n",
        "            # 移動先のファイル\n",
        "            path2= os.path.join(base_dir, tra_val_dir, classes[weed_name], file_name)\n",
        "            # ファイルを移動\n",
        "            new_path = shutil.move(path1, path2)\n",
        "            # ファイルの存在確認\n",
        "            print(os.path.exists(path2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89zSPJPufnsz"
      },
      "source": [
        "・trainデータ、validationデータ、predictionデータのgeneratorを作成します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ork4z7VlHv5T"
      },
      "source": [
        "import os \n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras.preprocessing.image as Image\n",
        "\n",
        "input_size = 224\n",
        "\n",
        "train_dir = \"./CLS/train\"\n",
        "validation_dir = \"./CLS/validation\"\n",
        "\n",
        "\n",
        "train_datagen = Image.ImageDataGenerator(\n",
        "            featurewise_center = False,\n",
        "            samplewise_center = False,\n",
        "            featurewise_std_normalization = False,\n",
        "            samplewise_std_normalization = False,\n",
        "            zca_whitening = False,\n",
        "            rotation_range = 90,\n",
        "            width_shift_range = 0.3,\n",
        "            height_shift_range = 0.3,\n",
        "            horizontal_flip = True,\n",
        "            vertical_flip = False,\n",
        "            rescale=1./255\n",
        "        )\n",
        "\n",
        "val_datagen = Image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "            train_dir,\n",
        "            target_size=(input_size,input_size),\n",
        "            batch_size=7,\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "            validation_dir,\n",
        "            target_size=(input_size,input_size),\n",
        "            batch_size=7,\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "print(\"データセット作成完了！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJCIFvFQ7TAj"
      },
      "source": [
        "### ■トレーニングの実行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2igLNwBzttgT"
      },
      "source": [
        "・モデルのレイヤー構成を定義します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dq94WdxULw9"
      },
      "source": [
        "\n",
        "#ファインチューニング+VGG+水増し。ここから実行してOK（VGG16をダウンロード）\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.optimizers import adam_v2\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.applications.vgg16 import VGG16\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import History, Callback\n",
        "# from keras.objectives import categorical_crossentropy\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "# from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from scipy.stats import mode\n",
        "import os, pickle\n",
        "\n",
        "\n",
        "def create_cnn():\n",
        "  input_size=224\n",
        "  #input_sizeは224,224までOK。\n",
        "\n",
        "  vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(input_size,input_size, 3))\n",
        "  last = vgg_conv.output\n",
        "\n",
        "  vgg_conv.trainable = True\n",
        "\n",
        "  set_trainable = False\n",
        "  for layer in vgg_conv.layers:\n",
        "    if layer.name == 'block5_conv1':\n",
        "      set_trainable = True\n",
        "    if set_trainable:\n",
        "      layer.trainable = True\n",
        "    else:\n",
        "      layer.trainable = False\n",
        "\n",
        "  mod = Flatten()(last)\n",
        "  mod = Dense(256, activation='relu')(mod)\n",
        "  #mod = Dropout(0.5)(mod)\n",
        "  preds = Dense(7, activation='softmax')(mod)\n",
        "\n",
        "  model = models.Model(vgg_conv.input, preds)\n",
        "\n",
        "  return model\n",
        "print(\"レイヤー構成を定義しました！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMCP246j39AB"
      },
      "source": [
        "・チェックポイントを定義します(val lossが一番低い値の時にweightファイルを保存)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa68Qz4p3_Ov"
      },
      "source": [
        "class Checkpoint(Callback):\n",
        "    def __init__(self, model, filepath):\n",
        "        self.model = model\n",
        "        self.filepath = filepath\n",
        "        self.best_val_acc = 0.0\n",
        "        self.best_val_loss = 0.7\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        # val_lossが最小の時ににweightを保存する\n",
        "        if self.best_val_loss > logs[\"val_loss\"]:\n",
        "            self.model.save_weights(self.filepath)\n",
        "            self.best_val_loss = logs[\"val_loss\"]\n",
        "            print(\"Weights saved.\", self.best_val_loss)\n",
        "print(\"チェックポイントを定義しました！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-DwQxyRfdxG"
      },
      "source": [
        "・学習の実行手順を定義します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLkWD57CZJEX"
      },
      "source": [
        "def train():\n",
        "    print(\"学習を開始します\")\n",
        "    hist = History()\n",
        "    train_model = create_cnn()\n",
        "    train_model.compile(optimizer=adam_v2.Adam(learning_rate=1e-5),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    cp = Checkpoint(train_model, f\"weights.hdf5\")\n",
        "    train_model.fit_generator(train_generator,epochs=10,validation_data=validation_generator,callbacks=[hist, cp])      \n",
        "    print(\"学習が完了しました\")\n",
        "    return hist.history\n",
        "print(\"実行手順を定義しました！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnbD3y_Zh7Pg"
      },
      "source": [
        "・学習を開始します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqaVKLXZgiEZ"
      },
      "source": [
        "K.clear_session()\n",
        "hist = train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTAmkFAX7qB3"
      },
      "source": [
        "### ■正解率と損失率をグラフ化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXMQMfNufvlL"
      },
      "source": [
        "・trainの正解率と損失率、validationの正解率と損失率をグラフ化します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tik0x-QsZKtC"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = hist\n",
        "\n",
        "acc=history['accuracy']\n",
        "val_acc=history['val_accuracy']\n",
        "loss=history['loss']\n",
        "val_loss=history['val_loss']\n",
        "\n",
        "epochs=range(1,len(acc)+1)\n",
        "\n",
        "#正解率plot\n",
        "plt.plot(epochs,acc,'bo',label='Training acc')\n",
        "plt.plot(epochs,val_acc,'b',label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "#損失値をplot\n",
        "plt.plot(epochs,loss,'bo',label='Training loss')\n",
        "plt.plot(epochs,val_loss,'b',label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBC6LNV8706s"
      },
      "source": [
        "### ■テスト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KRPYAVR_9Zn"
      },
      "source": [
        "・prediction用のデータセットを作成します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNZBgLlClER7"
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob\n",
        "import numpy as np\n",
        "from PIL import ImageFile\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES =True\n",
        "\n",
        "prediction_dir = \"./CLS/prediction\"\n",
        "prediction_classes = [\"grass\",\"hosoaogeitou\",\"ichibi\",\"kiku\",\"mameasagao\",\"oinutade\",\"shiroza\"]\n",
        "\n",
        "image_size = 224\n",
        "print(prediction_classes)\n",
        "X_test = []\n",
        "y_test = []\n",
        "for index, classlabel in enumerate(prediction_classes):\n",
        "    photos_dir = os.path.join(prediction_dir, classlabel)\n",
        "    files = glob.glob(photos_dir + \"/*.JPG\")\n",
        "    print(files)\n",
        "    for i, file in enumerate(files):\n",
        "        image = Image.open(file)\n",
        "        image = image.convert(\"RGB\")\n",
        "        image = image.resize((image_size, image_size))\n",
        "        data = np.asarray(image)\n",
        "        if i == 0:\n",
        "          print(data.shape)\n",
        "        X_test.append(data)\n",
        "        y_test.append(index)\n",
        "\n",
        "X_test1 = np.array(X_test)\n",
        "y_test1 = np.array(y_test)\n",
        "print(\"predictionデータ作成完了！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJiCyfYW8GKy"
      },
      "source": [
        "・混同行列表示用の関数を定義します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGZMSUefz-C5"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    # print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=-90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "print(\"混同行列表示用の関数を定義しました！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVEpU5mMEEOi"
      },
      "source": [
        "・保存したweightファイルをロードしてpredictionを行います\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ry5e6uAo9Hw"
      },
      "source": [
        "def sin_predict():\n",
        "    \n",
        "    X_test, y_test = X_test1, y_test1\n",
        "    X_test = X_test / 255.0\n",
        "    y_test_label = np.ravel(y_test)\n",
        "    y_test = to_categorical(y_test)\n",
        "  \n",
        "    train_model = create_cnn()\n",
        "    train_model.compile(optimizer=adam_v2.Adam(learning_rate=1e-5),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    # 最良のモデルの読み込み\n",
        "    train_model.load_weights(f\"weights.hdf5\")\n",
        "    for layer in train_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # 単体のテスト\n",
        "    single_pred = np.argmax(train_model.predict(X_test), axis=-1)\n",
        "\n",
        "    # テストのスコア\n",
        "    test_acc = accuracy_score(y_test, to_categorical(single_pred))\n",
        "\n",
        "    print(\"テストの結果は\", test_acc, \"です\")\n",
        "    \n",
        "    target_names = [\"grass\",\"hosoaogeitou\",\"ichibi\",\"kiku\",\"mameasagao\",\"oinutade\",\"shiroza\"]\n",
        "    cm = confusion_matrix(y_test_label, single_pred)\n",
        "    plot_confusion_matrix(cm, classes = target_names) \n",
        "    # print('Classification Report')\n",
        "    # print(classification_report(y_test_label, single_preds, target_names=target_names))\n",
        "print(\"テスト用の関数を定義しました！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkJ-M4D1ZJ4a"
      },
      "source": [
        "・テストを実行します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pMpwvF7qMTt"
      },
      "source": [
        "# テスト\n",
        "sin_predict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b340YPV35B--"
      },
      "source": [
        "### ■任意の写真をテストします"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "343MbhcH5lUv"
      },
      "source": [
        "・画像テスト用の関数を定義します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTKPufr6xSGa"
      },
      "source": [
        "def result_predict(path):\n",
        "    prediction_classes = [\"grass\",\"hosoaogeitou\",\"ichibi\",\"kiku\",\"mameasagao\",\"oinutade\",\"shiroza\"]\n",
        "\n",
        "    train_model = create_cnn()\n",
        "    train_model.compile(optimizer=adam_v2.Adam(learning_rate=1e-5),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    # 最良のモデルの読み込み\n",
        "    train_model.load_weights(f\"weights.hdf5\")\n",
        "    for layer in train_model.layers:\n",
        "        layer.trainable = False  \n",
        "    \n",
        "    X_test = []\n",
        "    image_size = 224\n",
        "    image = Image.open(path)\n",
        "    image = image.convert(\"RGB\")\n",
        "    image = image.resize((image_size, image_size))\n",
        "    data = np.asarray(image)\n",
        "    X_test.append(data)\n",
        "    X_test = np.array(X_test) / 255.0\n",
        "    result = np.argmax(train_model.predict(X_test), axis=-1)\n",
        "    print(\"雑草の種類は\", prediction_classes[result[0]], \"です\")\n",
        "print(\"画像テスト用の関数を定義しました！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gLtWRQr5Z_m"
      },
      "source": [
        "・predictionディレクトリから任意の写真を選択しpathを設定します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMTohBSJyDMc"
      },
      "source": [
        "image_dir = \"/content/CLS/prediction/grass/inubie_IMG_1505_19.JPG\"\n",
        "result_predict(image_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}